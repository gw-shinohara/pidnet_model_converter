# pidnet_to_onnx.py

import argparse
import os
import sys

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‘ã‚¹ã‚’è¿½åŠ  (å…ƒã®è¨­å®šã‚’ç¶­æŒ)
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

import torch
import torch.nn as nn
from torch.onnx import export
import onnx
import onnxruntime
import numpy as np

import models
from configs import config
from configs import update_config

def parse_args():
    parser = argparse.ArgumentParser(description='PIDNet to ONNX')

    parser.add_argument('--cfg',
                        help='experiment configure file name',
                        required=True,
                        type=str)
    parser.add_argument('--model-file',
                        help='Path to the trained PyTorch model file',
                        required=True,
                        type=str)
    parser.add_argument('--output-file',
                        help='Path to save the ONNX model',
                        default='pidnet.onnx',
                        type=str)
    # æ–°ã—ã„å¼•æ•°ã‚’è¿½åŠ : ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰
    parser.add_argument('--debug',
                        action='store_true',
                        help='Enable detailed ONNX mismatch analysis using strict tolerance (1e-5).')

    args = parser.parse_args()
    args.opts = []
    update_config(config, args)

    return args

def main():
    args = parse_args()

    torch.backends.cudnn.benchmark = True

    # --- 1. PyTorchãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ ---
    print("=> PyTorchãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    num_classes = 19
    model = models.pidnet.get_pred_model(config, num_classes)

    # å­¦ç¿’æ¸ˆã¿é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰
    if config.TEST.MODEL_FILE:
        model_state_file = config.TEST.MODEL_FILE
        print(f"   - é‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«: {model_state_file}")
        pretrained_dict = torch.load(model_state_file, map_location='cpu')
        if 'state_dict' in pretrained_dict:
            pretrained_dict = pretrained_dict['state_dict']

        model_dict = model.state_dict()
        pretrained_dict = {k: v for k, v in pretrained_dict.items()
                           if k in model_dict.keys()}

        model_dict.update(pretrained_dict)
        model.load_state_dict(model_dict)

    model.eval()
    print("=> ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰å®Œäº†ã€‚")

    # å…¥åŠ›ã‚µã‚¤ã‚ºã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—
    height, width = config.TEST.IMAGE_SIZE
    dummy_input = torch.randn(1, 3, height, width)

    # --- 2. ONNXã¸ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ ---
    output_file_path = args.output_file
    print(f"\n=> ONNXãƒ¢ãƒ‡ãƒ«ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­ ({output_file_path})...")

    try:
        # opuset_version 12 ã‚’ä½¿ç”¨
        export(model,
               dummy_input,
               output_file_path,
               export_params=True,
               opset_version=12,
               do_constant_folding=True,
               input_names=['input'],
               output_names=['output'],
               dynamic_axes={'input': {0: 'batch_size'},
                             'output': {0: 'batch_size'}})
        print("=> ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†ã€‚")
    except Exception as e:
        print(f"   - ã‚¨ãƒ©ãƒ¼: ONNXã¸ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚: {e}")
        return

    # --- 3. ONNXãƒ¢ãƒ‡ãƒ«ã®æ¤œè¨¼ (æ§‹é€ ãƒã‚§ãƒƒã‚¯) ---
    print("\n=> ONNXãƒ¢ãƒ‡ãƒ«ã‚’æ¤œè¨¼ä¸­...")
    try:
        onnx_model = onnx.load(output_file_path)
        onnx.checker.check_model(onnx_model)
        print("   - ONNXãƒ¢ãƒ‡ãƒ«ã®æ§‹é€ ã¯æ­£å¸¸ã§ã™ã€‚")
    except Exception as e:
        print(f"   - ã‚¨ãƒ©ãƒ¼: ONNXãƒ¢ãƒ‡ãƒ«ã®æ¤œè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸã€‚: {e}")
        return

    # --- 4. ONNX Runtimeã§ã®æœ€çµ‚æ¨è«–ã¨æ¯”è¼ƒ (å…ƒã®è¨±å®¹èª¤å·®) ---
    print("\n=> ONNX Runtimeã§ã®æœ€çµ‚æ¨è«–ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")
    try:
        ort_session = onnxruntime.InferenceSession(output_file_path)

        with torch.no_grad():
            torch_out = model(dummy_input)
            if isinstance(torch_out, tuple):
                torch_out = torch_out[0]

        ort_inputs = {ort_session.get_inputs()[0].name: dummy_input.numpy()}
        ort_outs = ort_session.run(None, ort_inputs)

        # å…ƒã®è¨±å®¹èª¤å·®ã§ãƒ†ã‚¹ãƒˆ
        rtol_orig, atol_orig = 1e-02, 1e-04
        print(f"   - PyTorchã¨ONNX Runtimeã®æœ€çµ‚å‡ºåŠ›ã‚’æ¯”è¼ƒä¸­ (rtol={rtol_orig}, atol={atol_orig})")
        np.testing.assert_allclose(torch_out.numpy(), ort_outs[0], rtol=rtol_orig, atol=atol_orig)
        print("   - PyTorchã¨ONNX Runtimeã®æ¨è«–çµæœãŒè¨±å®¹èª¤å·®å†…ã§ä¸€è‡´ã—ã¾ã—ãŸã€‚âœ…")

    except Exception as e:
        # --- 5. ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã§ã®ä¸­é–“ãƒãƒ¼ãƒ‰åˆ†æ ---
        print(f"   - ã‚¨ãƒ©ãƒ¼: æœ€çµ‚å‡ºåŠ›ã®æ¯”è¼ƒã«å¤±æ•—ã—ã¾ã—ãŸã€‚: {e}")
        if args.debug==True:
            print("\n=> ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰: ONNXã¨PyTorchã®ä¸­é–“ãƒãƒ¼ãƒ‰ã‚’è©³ç´°ã«åˆ†æä¸­...")
            print("ONNXDebuggerã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ")
            try:
                from utils.onnx_debugger import ONNXDebugger
            except ImportError:
                print("ERROR: onnx_debugger.py ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                print("         onnx_debugger.py ã¨ pidnet_to_onnx.py ã‚’åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®ã—ã¦ãã ã•ã„ã€‚")
            # ONNXDebuggerã‚’åˆæœŸåŒ–
            debugger = ONNXDebugger(model, output_file_path, dummy_input)

            # å³å¯†ãªèª¤å·®ã§åˆ†æã‚’é–‹å§‹ (rtol=1e-05, atol=1e-08)
            debugger.analyze(rtol=1e-05, atol=1e-08)

            # ONNXãƒãƒ¼ãƒ‰åã®ãƒªã‚¹ãƒˆã‚‚è¡¨ç¤º
            debugger.print_onnx_node_names()

        else:
            print("\nğŸ’¡ ãƒ’ãƒ³ãƒˆ: æœ€çµ‚æ¯”è¼ƒã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚è©³ç´°ãªåˆ†æã‚’è¡Œã†ã«ã¯ã€'--debug' ãƒ•ãƒ©ã‚°ã‚’ä»˜ã‘ã¦å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")


if __name__ == '__main__':
    main()
